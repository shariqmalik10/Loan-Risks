{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b0673e",
   "metadata": {},
   "source": [
    "# Introduction and Documentation for the Pipeline\n",
    "\n",
    "The goal of this project is to build a machine learning pipeline that will automate the process of retraining and fine-tuning the model every time we recieve new data\n",
    "\n",
    "The architecture for this pipeline has been divided into 6 sections: \n",
    "\n",
    "1. Data Ingestion \n",
    "2. Data Preprocessing \n",
    "3. Feature Engineering\n",
    "4. Model Training\n",
    "5. Model Evaluation \n",
    "6. Model Deployment (not implemented in this project)\n",
    "\n",
    "## Pipeline Designing \n",
    "\n",
    "Initially I was looking into complex ML pipeline architectures such as Directed Acyclic Graphs (DAG) as well as Single Leader Architecture but after reviewing the problem a few more times from the given question paper as well as the initial model that has been built in the 'lightgbm_build.ipynb' file, I realized that a simpler workflow would be much more suitable while also borrowing some concepts from DAG architecture. The next section is to show the initial and final draft for the pipeline design that I worked on\n",
    "\n",
    "## Drafts\n",
    "\n",
    "### Initial Draft Image \n",
    "\n",
    "<img src=\"../images/initial_draft.jpg\" alt=\"Initial Draft Image\" />\n",
    "\n",
    "### Final Draft Image \n",
    "\n",
    "<img src=\"../images/final_draft.jpg\" alt=\"Final Draft Image\" />\n",
    "\n",
    "### Discussion \n",
    "\n",
    "From the diagrams above, it can be observed that not much changed between my initial idea and the final design. The final draft image gives a high level overview of all the processes that happen within each step. To keep it simple I did not add any descriptions in the diagram as all the explanations and reasoning has been mentioned in the notebook along with the corresponding step. The pipeline overall, has been kept simple and designed in a way which allows for additional components as well as other pre-processing steps. Comments have been added at each important line of code in order to aid understanding of what the code is doing.  \n",
    "\n",
    "I have referred to the 'lightgbm_build.ipynb' as while working on that model I had already planned most of the steps that I needed to carry out for my pipeline. While designing the pipeline I needed to make some adjustments and change the placement of a few pre-processing steps that were carried out. Instead of saving the model in the final steps, I saved the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2eba4",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833be207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries for data exploration and preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "#library for graph plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "#library for model building \n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "#libraries to build pipeline \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#saving the pipeline\n",
    "import joblib\n",
    "\n",
    "import warnings as warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079c3f3",
   "metadata": {},
   "source": [
    "# Data Ingestion \n",
    "\n",
    "In this section we are going to be reading data from the file 'loan.csv' which will be used in the retraining of the model. Do note that we make the assumption that <b>the only file that is changed is 'loan.csv'</b> so each time the file is changed we will run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ae78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data file from Notebook 1\n",
    "file_path = '../data/loan.csv'\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, parse_dates=['applicationDate', 'originatedDate'])\n",
    "    \n",
    "    #load initial model \n",
    "    model_path = \"models/\"\n",
    "    with open(model_path + 'initial_model.pkl', 'rb') as file:\n",
    "        loaded_model = pickle.load(file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1766116d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/initial_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#load initial model \u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minitial_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      9\u001b[0m     loaded_model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/initial_model.pkl'"
     ]
    }
   ],
   "source": [
    "data = load_data(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4b16b",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Within this section, we are going to carry out all the data processing in steps as shown below:\n",
    "1. Filtering out for records by applications which are funded\n",
    "2. Imputing missing/NA values with 0 for 'nPaidOff'\n",
    "3. Encoding 'payFrequency'\n",
    "4. Encoding 'loanStatus'\n",
    "5. One-Hot encoding 'leadType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"*\"*50)\n",
    "        print(\">>>> Pipeline Started\\n\")\n",
    "        print(\">>>>>>> Data Preprocessing\\n\")\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        print(\">>>> fit called\")\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.query('isFunded==1')\n",
    "        #fill in nPaidOff with 0\n",
    "        X_cp = X.copy()\n",
    "        X['nPaidOff'] = X_cp['nPaidOff'].fillna(0)\n",
    "        \n",
    "        #encode payFrequency\n",
    "        new_vals = {\n",
    "            'payFrequency': {'B': 0, 'W': 1, 'M': 2, 'S': 3, 'I': 4}\n",
    "        }\n",
    "        X = X.replace(new_vals)\n",
    "        \n",
    "        #encode loanStatus\n",
    "        valid_group = ['Paid Off Loan', 'Settlement Paid Off', 'Settled Bankruptcy', 'Charged Off']\n",
    "        #remove any entry not from the defined 4 \n",
    "        X = X[X.loanStatus.isin(valid_group) != False]\n",
    "        new_loanstatus = {\n",
    "            'loanStatus': {'Paid Off Loan': 1, 'Settlement Paid Off': 1, 'Settled Bankruptcy': 0, 'Charged Off': 0}\n",
    "        }\n",
    "        X = X.replace(new_loanstatus)\n",
    "        \n",
    "        #one hot encoding on leadType\n",
    "        X = pd.get_dummies(X, columns=['leadType'])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        print(\">>>> Model Fitting\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba1684",
   "metadata": {},
   "source": [
    "# Feature Engineering \n",
    "\n",
    "In this section I carried out the following feature engineering steps:\n",
    "\n",
    "1. Feature Creation: Created a new feature, 'processTime', which is used to improve the model's overall capability to predict loan application risks\n",
    "2. Feature Selection: Dropped/removed irrelevant columns such as 'approved', 'isFunded' as they were not needed given the fact that none of them had any direct links to how they could affect the locan application risks \n",
    "3. Scaling: Applied min-max scaling and box-cox to respective columns. \n",
    "4. Sampling: I used over-sampling to balance out the class distribution of the training set as it would otherwise result in the model being heavily biased given the fact that around 97% of the set was for class '1' and a mere 3% was for class '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb535963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\">>>> Feature Engineering\\n\")\n",
    "        \n",
    "    \n",
    "    def fit(self, X):\n",
    "        print(\">>>> Feature Engineering fit called\")\n",
    "        \n",
    "    def transform(self, X):\n",
    "        #add process time\n",
    "        X['processTime'] = (X['originatedDate'] - X['applicationDate']).dt.total_seconds() / 3600\n",
    "        \n",
    "        #drop irrelevant columns\n",
    "        X.drop(['originated', 'isFunded', 'approved', 'originatedDate', \n",
    "                 'applicationDate', 'fpStatus', 'state'], axis=\"columns\", inplace=True)\n",
    "        \n",
    "        #applying min-max scaling to loanAmount and originallyScheduledPaymentAmount\n",
    "        scaler = MinMaxScaler()\n",
    "        X[['loanAmount', 'originallyScheduledPaymentAmount']] = scaler.fit_transform(\n",
    "            X[['loanAmount', 'originallyScheduledPaymentAmount']])\n",
    "\n",
    "        #applying box-cox to leadCost and nPaidOff as both of their data distribution is heavily concentrated near 0 and is positively skewed\n",
    "        X['nPaidOff'], _ = boxcox(X['nPaidOff'] + 1)\n",
    "        X['leadCost'], _ = boxcox(X['leadCost'] + 1)\n",
    "        \n",
    "        #split data into x and y.\n",
    "        #drop the unique identifiers\n",
    "        X , y = X.drop(['loanId', 'anon_ssn', 'loanStatus', 'clarityFraudId'], axis=1), X[\"loanStatus\"]\n",
    "        ros = RandomOverSampler(random_state=8)\n",
    "        X, y = ros.fit_resample(X, y)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        print(\">>>> Model Fitting\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ca66f",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this section I build and fit the model on the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c98fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrain(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\">>>> Model Training\\n\")\n",
    "    \n",
    "    def fit(self, X):\n",
    "        print(\">>>> Model Fitting\\n\")\n",
    "        print(\"*\"*50)\n",
    "        print(\">>>> Model has been built \")\n",
    "        #unpack the values from the previous transformer\n",
    "        X, y = X\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8)\n",
    "        #building the model \n",
    "        model = lgb.LGBMClassifier(learning_rate=0.09,max_depth=-5,random_state=42)\n",
    "        model.fit(X_train,y_train,eval_set=[(X_test,y_test),(X_train,y_train)],\n",
    "                  verbose=20,eval_metric='auc')\n",
    "        \n",
    "        return model, X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocess_data\", DataPreprocessing()),\n",
    "    (\"feature_enginner\", FeatureEngineering()),\n",
    "    (\"model_train\", ModelTrain())\n",
    "])\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model, X_train, X_test, y_train, y_test = pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed789d0",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "As discussed in the previous notebook, we cannot use accuracy to judge whether the model is performing well due to the class distribution inequality  \n",
    "So here I calculated the AUC score for the model to check how good the model is performing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47df03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the auc score \n",
    "print(\"*\"*50)\n",
    "AUC = metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "print(f\"Initial Model AUC: {AUC*100: .2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbc2440",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning\n",
    "\n",
    "For model fine-tuning, I used GridSearchCV to find the best parameters for the model in order to improve its performance. However to save computational resources, I added a check so that model fine-tuning would only happen if the model's current AUC score was less than 95%. This is because a score of 95% or above indicates that the model can perform very well already so no need to fine tune any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the gridsearch function \n",
    "\n",
    "def grid_search():\n",
    "    #initializing a LightGBM model and calling GridSearch on it \n",
    "    new_model = lgb.LGBMClassifier(random_state=40)\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.1, 0.09, 0.05, 0.01],\n",
    "        'max_depth': [-1, 3, 5, 7, 10],\n",
    "        'num_leaves': [20, 30, 40, 50, 75, 100]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(new_model, param_grid, cv=5, scoring=\"roc_auc\")\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params = grid_result.best_params_\n",
    "    best_score = grid_result.best_score_\n",
    "    \n",
    "    return grid_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only undergo grid search if the model's auc is less than 95% otherwise it is a waste of computational resources \n",
    "threshold = 0.95\n",
    "if AUC < threshold:\n",
    "    improved = grid_search()\n",
    "    model = lgb.LGBMClassifier(**improved.best_params_)\n",
    "    model.fit(X_train, y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best model, its params and the pipeline \n",
    "print(\"*\"*50)\n",
    "AUC = metrics.roc_auc_score(y_test, model.predict(X_test))\n",
    "print(f\"Model AUC: {AUC*100: .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebe5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire pipeline\n",
    "joblib.dump(pipeline, 'pipelines/trained_pipeline.joblib')\n",
    "\n",
    "#save the latest model as reference\n",
    "joblib.dump(model, 'models/latest_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417dedbb",
   "metadata": {},
   "source": [
    "# Notes and Future Improvements \n",
    "\n",
    "From this entire process I took a few notes and points that I would like to mention here:\n",
    "- A lot of the parts of data pre-processing were implemented while making the assumption that aside from the column 'nPaidOff', each column would have data entries in them. So the improvement to make here would be to later on add in a few more pre-procesing steps that can impute data for other features that may not have entries within them \n",
    "- In the feature-engineering step, I had a vague idea on another feature which could be created but I was not confident and too clear as to whether it would actually help in improving the created model's performance to predict loan application risks. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
